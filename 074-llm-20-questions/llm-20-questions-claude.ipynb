{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc5e605b-1308-4f8d-b08a-12ac2bc54300",
   "metadata": {},
   "source": [
    "# Create a 20 Questions Answer with Rigging\n",
    "- Claude 3 Version!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "71effb4f-ed9c-451c-b4e0-11c656a63ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "                <script type=\"application/javascript\" id=\"jupyter_black\">\n",
       "                (function() {\n",
       "                    if (window.IPython === undefined) {\n",
       "                        return\n",
       "                    }\n",
       "                    var msg = \"WARNING: it looks like you might have loaded \" +\n",
       "                        \"jupyter_black in a non-lab notebook with \" +\n",
       "                        \"`is_lab=True`. Please double check, and if \" +\n",
       "                        \"loading with `%load_ext` please review the README!\"\n",
       "                    console.log(msg)\n",
       "                    alert(msg)\n",
       "                })()\n",
       "                </script>\n",
       "                "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import jupyter_black\n",
    "\n",
    "jupyter_black.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fda2142b-2abf-47a3-bffb-16125650ca10",
   "metadata": {},
   "source": [
    "# Question Asker Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64dcc698-b43d-44e7-978e-f495de2104b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import rigging as rg\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "\n",
    "class AskerQuestion(rg.Model):\n",
    "    question: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "630bb670-9063-43c9-b765-5789f8c57f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"claude-3-sonnet-20240229\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e892208-68e1-4d5a-9602-8fd564465405",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = []\n",
    "answers = []\n",
    "\n",
    "\n",
    "def ask_next_question(questions, answers, verbose=False):\n",
    "    system_prompt = \"\"\"\n",
    "        You are playing the game 20 questions.\n",
    "            - Your task is to ask questions.\n",
    "            - These questions should try to quickly find the keyword.\n",
    "            - You will recieve back only yes or no responses.\n",
    "            - Note previous questions and their answers when creating your next question\n",
    "            - Do not assume anything specific too soon. Ask broad questions that could divide all possible remaining things into an even split.\n",
    "            - The keyword will be a person, place or thing.\n",
    "    \"\"\"\n",
    "    assert len(questions) == len(answers)\n",
    "\n",
    "    prev_content = \"\"\n",
    "    for i, question in enumerate(questions):\n",
    "        prev_content += f\"question {i} <question>{question}</question> <answer>{answers[i]}</answer>\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        Previous questions and answers are:\n",
    "            {prev_content}\n",
    "        \n",
    "        Ask your question within the tag {AskerQuestion.xml_tags()}\n",
    "    \"\"\"\n",
    "    asker = (\n",
    "        rg.get_generator(BASE_MODEL)\n",
    "        .chat(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "        )\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "    question = asker.last.parse(AskerQuestion).question\n",
    "    if verbose:\n",
    "        print(f\"=== Question {len(questions) + 1} ====\")\n",
    "        print(question)\n",
    "\n",
    "    return question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2e5e20d-71d2-4270-abe7-f5205c9460f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = ask_next_question(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "55910283-d35b-4af3-9574-3ee4bcc268a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is the keyword a living thing?\n"
     ]
    }
   ],
   "source": [
    "print(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed3f01f-1aa6-4ba5-883b-bf9729fd82f3",
   "metadata": {},
   "source": [
    "# LLM Trying to Guess the Keyword \"TOMATO\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f03b2db8-2c83-432c-838a-44a0b118e099",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"no\"\n",
    "questions.append(question)\n",
    "answers.append(answer)\n",
    "question = ask_next_question(questions, answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5f7dbc91-f342-4937-a2ff-b5b41f8854a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = \"no, its a red fruit used in sauses with spagetti\"\n",
    "questions.append(question)\n",
    "answers.append(answer)\n",
    "question = ask_next_question(questions, answers)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77947f00-12d2-4632-a55c-a61925273a07",
   "metadata": {},
   "source": [
    "# Create our Answerer Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ef894e7-d4a0-4610-840c-0714b693892b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class YesNoAnswer(rg.Model):\n",
    "    \"Yes/No answer answer with coercion\"\n",
    "    answer: str\n",
    "\n",
    "\n",
    "def get_agent_answer(keyword, question):\n",
    "\n",
    "    system_prompt = f\"\"\"\n",
    "        You are playing the game 20 questions. You will be asked a question an must answer yes or no.\n",
    "        The keyword you are answering for is {keyword}\n",
    "    \"\"\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        The question is <question>{question}</question>\n",
    "    \n",
    "        Your response should be given between the tags {YesNoAnswer.xml_tags()}\n",
    "    \n",
    "        It should only be 'yes' or 'no'\n",
    "        \n",
    "        \"\"\"\n",
    "\n",
    "    answerer = (\n",
    "        rg.get_generator(BASE_MODEL)\n",
    "        .chat(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "        )\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "    answer = answerer.last.parse(YesNoAnswer).answer\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f01793bc-da5f-46c5-a1be-0af71b6f75fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'yes'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword = \"Tom Hanks\"\n",
    "question = \"Is he a handsome lad?\"\n",
    "get_agent_answer(keyword, question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb940742-0bb6-4d18-b239-7dcd5dfb0650",
   "metadata": {},
   "source": [
    "# Guesser Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30c4b915-d60f-4a0e-89f9-d10e07eb0915",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Guess(rg.Model):\n",
    "    \"20 Questions guess\"\n",
    "    guess: str\n",
    "\n",
    "\n",
    "def make_guess(questions: list, answers: list, guesses: list, verbose=False):\n",
    "    system_prompt = \"\"\"\n",
    "        You are playing the game 20 questions.\n",
    "            - Your task is to guess the keyword.\n",
    "            - Note previous questions and their answers when creating your guess\n",
    "            - The keyword will be a person, place or thing.\n",
    "    \"\"\"\n",
    "    assert len(questions) == len(answers)\n",
    "\n",
    "    prev_content = \"\"\n",
    "    for i, question in enumerate(questions):\n",
    "        prev_content += f\"question {i} <question>{question}</question> <answer>{answers[i]}</answer>\"\n",
    "\n",
    "    user_prompt = f\"\"\"\n",
    "        Previous questions and answers are:\n",
    "            {prev_content}\n",
    "\n",
    "        Previous Guesses are, do not repeat these:\n",
    "            {','.join(guesses)}\n",
    "        \n",
    "        You will now guess for the 20 question game.\n",
    "          - Be specific.\n",
    "          - Guess only a single thing.\n",
    "        \n",
    "          \n",
    "        Put the word for your guess in the following tag {Guess.xml_tags()}\n",
    "    \"\"\"\n",
    "    asker = (\n",
    "        rg.get_generator(BASE_MODEL)\n",
    "        .chat(\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ]\n",
    "        )\n",
    "        .run()\n",
    "    )\n",
    "\n",
    "    guess = asker.last.parse(Guess).guess\n",
    "    if verbose:\n",
    "        print(f\"=== Guess {len(questions) + 1} ====\")\n",
    "        print(guess)\n",
    "\n",
    "    return guess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6b57ddc-cdc3-413b-a3ce-626b23387193",
   "metadata": {},
   "source": [
    "# Lets let the Agents fight it out!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3e7e5bf7-e54d-4420-8265-8e8d4a6dcba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "def keyword_guessed(guess: str, keyword: str) -> bool:\n",
    "    def normalize(s: str) -> str:\n",
    "        t = str.maketrans(\"\", \"\", string.punctuation)\n",
    "        return s.lower().replace(\"the\", \"\").replace(\" \", \"\").translate(t)\n",
    "\n",
    "    if normalize(guess) == normalize(keyword):\n",
    "        return True\n",
    "    # for s in alts:\n",
    "    #   if normalize(s) == normalize(guess):\n",
    "    #     return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77e44580-a445-4a4a-bd82-aadd0df04c90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Round 0\n",
      "[Question]: Is the keyword a living thing?\n",
      "[Answer]:   yes\n",
      "[Guess]: animal\n",
      "====================\n",
      "Round 1\n",
      "[Question]: Is the keyword an animal?\n",
      "[Answer]:   no\n",
      "[Guess]: human\n",
      "====================\n",
      "Round 2\n",
      "[Question]: Is the keyword a plant?\n",
      "[Answer]:   yes\n",
      "[Guess]: Tree\n",
      "====================\n",
      "Round 3\n",
      "[Question]: Is the keyword a tree?\n",
      "[Answer]:   no\n",
      "[Guess]: Flower\n",
      "====================\n",
      "Round 4\n",
      "[Question]: Is the keyword a flowering plant?\n",
      "[Answer]:   yes\n",
      "[Guess]: Rose\n",
      "====================\n",
      "Round 5\n",
      "[Question]: Is the keyword a vegetable?\n",
      "[Answer]:   no\n",
      "[Guess]: Tulip\n",
      "====================\n",
      "Round 6\n",
      "[Question]: Is the keyword a fruit?\n",
      "[Answer]:   yes\n",
      "[Guess]: Apple\n",
      "====================\n",
      "Round 7\n",
      "[Question]: Is the keyword a tropical fruit?\n",
      "[Answer]:   no\n",
      "[Guess]: Grape\n",
      "====================\n",
      "Round 8\n",
      "[Question]: Is the keyword a citrus fruit?\n",
      "[Answer]:   no\n",
      "[Guess]: Strawberry\n",
      "====================\n",
      "Round 9\n",
      "[Question]: Is the keyword a berry?\n",
      "[Answer]:   yes\n",
      "[Guess]: Blueberry\n",
      "====================\n",
      "Round 10\n",
      "[Question]: Is the keyword a strawberry?\n",
      "[Answer]:   no\n",
      "[Guess]: Raspberry\n",
      "====================\n",
      "Round 11\n",
      "[Question]: Is the keyword a blueberry?\n",
      "[Answer]:   no\n",
      "[Guess]: Blackberry\n",
      "====================\n",
      "Round 12\n",
      "[Question]: Is the keyword a raspberry?\n",
      "[Answer]:   no\n",
      "[Guess]: Cranberry\n",
      "====================\n",
      "Round 13\n",
      "[Question]: Is the keyword a blackberry?\n",
      "[Answer]:   no\n",
      "[Guess]: Cranberry\n",
      "====================\n",
      "Round 14\n",
      "[Question]: Is the keyword a cranberry?\n",
      "[Answer]:   no\n",
      "[Guess]: Gooseberry\n",
      "====================\n",
      "Round 15\n",
      "[Question]: Is the keyword a grape?\n",
      "[Answer]:   no\n",
      "[Guess]: Blackcurrant\n",
      "====================\n",
      "Round 16\n",
      "[Question]: Is the keyword a red berry?\n",
      "[Answer]:   yes\n",
      "\n",
      "\u001b[1;31mGive Feedback / Get Help: https://github.com/BerriAI/litellm/issues/new\u001b[0m\n",
      "LiteLLM.Info: If you need to debug this error, use `litellm.set_verbose=True'.\n",
      "\n"
     ]
    },
    {
     "ename": "RateLimitError",
     "evalue": "AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"rate_limit_error\",\"message\":\"Number of requests has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.\"}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnthropicError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/main.py:1197\u001b[0m, in \u001b[0;36mcompletion\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   1191\u001b[0m     api_base \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   1192\u001b[0m         api_base\n\u001b[1;32m   1193\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m litellm\u001b[38;5;241m.\u001b[39mapi_base\n\u001b[1;32m   1194\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m get_secret(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mANTHROPIC_API_BASE\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1195\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://api.anthropic.com/v1/messages\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1196\u001b[0m     )\n\u001b[0;32m-> 1197\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43manthropic_chat_completions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompletion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1198\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1199\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmessages\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmessages\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_base\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_base\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1201\u001b[0m \u001b[43m        \u001b[49m\u001b[43macompletion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43macompletion\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcustom_prompt_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1203\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_response\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_response\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1204\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprint_verbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprint_verbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1205\u001b[0m \u001b[43m        \u001b[49m\u001b[43moptional_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moptional_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1206\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlitellm_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlitellm_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1207\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogger_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger_fn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1208\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# for calculating input/output tokens\u001b[39;49;00m\n\u001b[1;32m   1209\u001b[0m \u001b[43m        \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1210\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlogging_obj\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogging\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1211\u001b[0m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1212\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1213\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optional_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstream\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;129;01mor\u001b[39;00m acompletion \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m   1214\u001b[0m     \u001b[38;5;66;03m## LOGGING\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/llms/anthropic.py:486\u001b[0m, in \u001b[0;36mAnthropicChatCompletion.completion\u001b[0;34m(self, model, messages, api_base, custom_prompt_dict, model_response, print_verbose, encoding, api_key, logging_obj, optional_params, acompletion, litellm_params, logger_fn, headers)\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m response\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m200\u001b[39m:\n\u001b[0;32m--> 486\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m AnthropicError(\n\u001b[1;32m    487\u001b[0m                 status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, message\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mtext\n\u001b[1;32m    488\u001b[0m             )\n\u001b[1;32m    489\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_response(\n\u001b[1;32m    490\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m    491\u001b[0m     response\u001b[38;5;241m=\u001b[39mresponse,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    499\u001b[0m     print_verbose\u001b[38;5;241m=\u001b[39mprint_verbose,\n\u001b[1;32m    500\u001b[0m )\n",
      "\u001b[0;31mAnthropicError\u001b[0m: {\"type\":\"error\",\"error\":{\"type\":\"rate_limit_error\",\"message\":\"Number of requests has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.\"}}",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRateLimitError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_248931/1861158559.py\u001b[0m in \u001b[0;36m?\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[Question]: {question}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[Answer]:   {answer}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mquestions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestion\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[0manswers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_guess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mguesses\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m     \u001b[0mguesses\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'[Guess]: {guess}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkeyword_guessed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeyword\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_248931/3578688539.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(questions, answers, guesses, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msystem_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m                 \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"user\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0muser_prompt\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m             ]\n\u001b[1;32m     40\u001b[0m         )\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     )\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mguess\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0masker\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlast\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mGuess\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mguess\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/rigging/chat.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mReturns\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m             \u001b[0mThe\u001b[0m \u001b[0mgenerated\u001b[0m \u001b[0mChat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m         \"\"\"\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_many\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/rigging/chat.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, count, params, skip_failed)\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocessor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mstate\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m         \u001b[0mpending_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    947\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mpending_states\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 948\u001b[0;31m             inbounds = self.generator.generate_messages(\n\u001b[0m\u001b[1;32m    949\u001b[0m                 \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessages\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpending_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpending_states\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprefix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mall\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    950\u001b[0m             )\n\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/rigging/generator.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, messages, params, prefix)\u001b[0m\n\u001b[1;32m    526\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m         \u001b[0mgenerated\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0m_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstrict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mtrace_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Messages {i+1}/{len(messages)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mnext_message\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_generate_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_messages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mgenerated\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0mtrace_messages\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnext_message\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34mf\"Response {i+1}/{len(messages)}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/rigging/generator.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(self, messages, params)\u001b[0m\n\u001b[1;32m    485\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_generate_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mMessage\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mGenerateParams\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMessage\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m         result = litellm.completion(\n\u001b[0m\u001b[1;32m    487\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m             \u001b[0;34m[\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minclude\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmessage\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmessages\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m             \u001b[0mapi_key\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapi_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2972\u001b[0m                     if (\n\u001b[1;32m   2973\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m   2975\u001b[0m                         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2976\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   2972\u001b[0m                     if (\n\u001b[1;32m   2973\u001b[0m                         \u001b[0mliteDebuggerClient\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mliteDebuggerClient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdashboard_url\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2974\u001b[0m                     ):  # make it easy to get to the debugger logs if you've initialized it\n\u001b[1;32m   2975\u001b[0m                         \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\"\\n Check the log in your dashboard - {liteDebuggerClient.dashboard_url}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2976\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/main.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, messages, timeout, temperature, top_p, n, stream, stop, max_tokens, presence_penalty, frequency_penalty, logit_bias, user, response_format, seed, tools, tool_choice, logprobs, top_logprobs, deployment_id, extra_headers, functions, function_call, base_url, api_version, api_key, model_list, **kwargs)\u001b[0m\n\u001b[1;32m   2141\u001b[0m             )\n\u001b[1;32m   2142\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2143\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2144\u001b[0m         \u001b[0;31m## Map to OpenAI Exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2145\u001b[0;31m         raise exception_type(\n\u001b[0m\u001b[1;32m   2146\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2147\u001b[0m             \u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcustom_llm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2148\u001b[0m             \u001b[0moriginal_exception\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   8699\u001b[0m         ):\n\u001b[1;32m   8700\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_all_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8701\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8704\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8705\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2023/envs/dreadnode/lib/python3.10/site-packages/litellm/utils.py\u001b[0m in \u001b[0;36m?\u001b[0;34m(model, original_exception, custom_llm_provider, completion_kwargs, extra_kwargs)\u001b[0m\n\u001b[1;32m   8699\u001b[0m         ):\n\u001b[1;32m   8700\u001b[0m             \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mThread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mget_all_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mllm_provider\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8701\u001b[0m         \u001b[0;31m# don't let an error with mapping interrupt the user from receiving an error from the llm api calls\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8702\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexception_mapping_worked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 8703\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8704\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8705\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0moriginal_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRateLimitError\u001b[0m: AnthropicException - {\"type\":\"error\",\"error\":{\"type\":\"rate_limit_error\",\"message\":\"Number of requests has exceeded your per-minute rate limit (https://docs.anthropic.com/en/api/rate-limits); see the response headers for current usage. Please try again later or contact sales at https://www.anthropic.com/contact-sales to discuss your options for a rate limit increase.\"}}"
     ]
    }
   ],
   "source": [
    "keyword = 'Tomato'\n",
    "questions = []\n",
    "answers = []\n",
    "guesses = []\n",
    "max_questions = 20\n",
    "for i in range(max_questions):\n",
    "    question = ask_next_question(questions, answers)\n",
    "    answer = get_agent_answer(keyword, question)\n",
    "    print('=' * 20)\n",
    "    print(f'Round {i}')\n",
    "    print(f'[Question]: {question}')\n",
    "    print(f'[Answer]:   {answer}')\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    guess = make_guess(questions, answers, guesses)\n",
    "    guesses.append(guess)\n",
    "    print(f'[Guess]: {guess}')\n",
    "    if keyword_guessed(guess, keyword):\n",
    "        print('FOUND IT!!!! BIZNATCH!!!!!!')\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e65e3201-bbc3-4f08-9853-271bfd083c87",
   "metadata": {},
   "source": [
    "# Can Claude Guess Pizza in 20 Questions?!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7ba656c2-47f1-4fc2-b849-c57a0fe484ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6667067d-0a57-4aae-81ba-224d435174d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Round 1\n",
      "[Question]: Is the keyword a living thing?\n",
      "[Answer]:   no\n",
      "[Guess]: Mountain\n",
      "====================\n",
      "Round 2\n",
      "[Question]: Is the keyword a physical object?\n",
      "[Answer]:   yes\n",
      "[Guess]: Smartphone\n",
      "====================\n",
      "Round 3\n",
      "[Question]: Is the keyword something man-made?\n",
      "[Answer]:   yes\n",
      "[Guess]: Computer\n",
      "====================\n",
      "Round 4\n",
      "[Question]: Is the keyword something portable or movable?\n",
      "[Answer]:   yes\n",
      "[Guess]: Laptop\n",
      "====================\n",
      "Round 5\n",
      "[Question]: Is the keyword some kind of tool or utensil?\n",
      "[Answer]:   no\n",
      "[Guess]: Car\n",
      "====================\n",
      "Round 6\n",
      "[Question]: Is the keyword some kind of electronic device?\n",
      "[Answer]:   no\n",
      "[Guess]: Suitcase\n",
      "====================\n",
      "Round 7\n",
      "[Question]: Is the keyword a vehicle?\n",
      "[Answer]:   no\n",
      "[Guess]: Bicycle\n",
      "====================\n",
      "Round 8\n",
      "[Question]: Is the keyword some kind of toy or game?\n",
      "[Answer]:   no\n",
      "[Guess]: Book\n",
      "====================\n",
      "Round 9\n",
      "[Question]: Is the keyword a piece of furniture?\n",
      "[Answer]:   no\n",
      "[Guess]: Guitar\n",
      "====================\n",
      "Round 10\n",
      "[Question]: Is the keyword some kind of clothing or accessory?\n",
      "[Answer]:   no\n",
      "[Guess]: Backpack\n",
      "====================\n",
      "Round 11\n",
      "[Question]: Is the keyword some kind of container or vessel?\n",
      "[Answer]:   no\n",
      "[Guess]: Ball\n",
      "====================\n",
      "Round 12\n",
      "[Question]: Is the keyword a writing implement like a pen or pencil?\n",
      "[Answer]:   no\n",
      "[Guess]: Umbrella\n",
      "====================\n",
      "Round 13\n",
      "[Question]: Is the keyword some kind of decorative or ornamental object?\n",
      "[Answer]:   no\n",
      "[Guess]: Chair\n",
      "====================\n",
      "Round 14\n",
      "[Question]: Is the keyword some kind of sporting equipment or related to sports or exercise?\n",
      "[Answer]:   no\n",
      "[Guess]: Luggage\n",
      "====================\n",
      "Round 15\n",
      "[Question]: Is the keyword some kind of building material like bricks, lumber, pipes, etc.?\n",
      "[Answer]:   no\n",
      "[Guess]: Key\n",
      "====================\n",
      "Round 16\n",
      "[Question]: Is the keyword some kind of office or school supply like paper, notebooks, staplers, etc.?\n",
      "[Answer]:   no\n",
      "[Guess]: Wallet\n",
      "====================\n",
      "Round 17\n",
      "[Question]: Is the keyword some kind of musical instrument?\n",
      "[Answer]:   no\n",
      "[Guess]: Briefcase\n",
      "====================\n",
      "Round 18\n",
      "[Question]: Is the keyword some kind of household item or appliance used for cleaning, cooking, laundry or other domestic purposes?\n",
      "[Answer]:   yes\n",
      "[Guess]: Vacuum cleaner\n",
      "====================\n",
      "Round 19\n",
      "[Question]: Is the keyword a kitchen appliance or utensil?\n",
      "[Answer]:   no\n",
      "[Guess]: Iron\n",
      "====================\n",
      "Round 20\n",
      "[Question]: Is the keyword some kind of vacuum cleaner or other cleaning appliance?\n",
      "[Answer]:   no\n",
      "[Guess]: Washing Machine\n"
     ]
    }
   ],
   "source": [
    "keyword = \"Pizza\"\n",
    "questions = []\n",
    "answers = []\n",
    "guesses = []\n",
    "max_questions = 20\n",
    "\n",
    "for i in range(max_questions):\n",
    "    sleep(5)\n",
    "    question = ask_next_question(questions, answers)\n",
    "    answer = get_agent_answer(keyword, question)\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Round {i+1}\")\n",
    "    print(f\"[Question]: {question}\")\n",
    "    print(f\"[Answer]:   {answer}\")\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    guess = make_guess(questions, answers, guesses)\n",
    "    guesses.append(guess)\n",
    "    print(f\"[Guess]: {guess}\")\n",
    "    if keyword_guessed(guess, keyword):\n",
    "        print(\"FOUND IT!!!! BIZNATCH!!!!!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fdbfbe-f706-4fac-b9c8-33772daa8684",
   "metadata": {},
   "source": [
    "# Tom HANKS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "83aa1a56-42b1-46f6-a32f-8a83e67c76c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================\n",
      "Round 1\n",
      "[Question]: Is the keyword a person?\n",
      "[Answer]:   yes\n",
      "[Guess]: Abraham Lincoln\n",
      "====================\n",
      "Round 2\n",
      "[Question]: Is this person a famous historical figure?\n",
      "[Answer]:   no\n",
      "[Guess]: Mom\n",
      "====================\n",
      "Round 3\n",
      "[Question]: Is this person alive today?\n",
      "[Answer]:   yes\n",
      "[Guess]: Barack Obama\n",
      "====================\n",
      "Round 4\n",
      "[Question]: Is this person a celebrity or public figure?\n",
      "[Answer]:   yes\n",
      "[Guess]: Elon Musk\n",
      "====================\n",
      "Round 5\n",
      "[Question]: Is this person involved in entertainment or media?\n",
      "[Answer]:   yes\n",
      "[Guess]: Taylor Swift\n",
      "====================\n",
      "Round 6\n",
      "[Question]: Is this person an actor or actress?\n",
      "[Answer]:   yes\n",
      "[Guess]: Jennifer Lawrence\n",
      "====================\n",
      "Round 7\n",
      "[Question]: Is this person primarily known for films or television?\n",
      "[Answer]:   yes\n",
      "[Guess]: Tom Hanks\n",
      "FOUND IT!!!! BIZNATCH!!!!!!\n"
     ]
    }
   ],
   "source": [
    "keyword = \"Tom Hanks\"\n",
    "questions = []\n",
    "answers = []\n",
    "guesses = []\n",
    "max_questions = 20\n",
    "\n",
    "for i in range(max_questions):\n",
    "    sleep(5)\n",
    "    question = ask_next_question(questions, answers)\n",
    "    answer = get_agent_answer(keyword, question)\n",
    "    print(\"=\" * 20)\n",
    "    print(f\"Round {i+1}\")\n",
    "    print(f\"[Question]: {question}\")\n",
    "    print(f\"[Answer]:   {answer}\")\n",
    "    questions.append(question)\n",
    "    answers.append(answer)\n",
    "    guess = make_guess(questions, answers, guesses)\n",
    "    guesses.append(guess)\n",
    "    print(f\"[Guess]: {guess}\")\n",
    "    if keyword_guessed(guess, keyword):\n",
    "        print(\"FOUND IT!!!! BIZNATCH!!!!!!\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "468e6379-1c0f-4fe5-a7e1-42d692bfd875",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
